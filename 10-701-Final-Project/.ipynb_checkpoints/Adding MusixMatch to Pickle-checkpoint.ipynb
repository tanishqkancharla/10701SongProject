{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import scipy.io as sio\n",
    "import hdf5_getters\n",
    "from pandas import read_hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Need to include track_id in pickle file #######\n",
    "finalWorkingDF = pd.read_pickle(\"finalWorkingDF\")\n",
    "#finalWorkingDF.to_pickle(\"musicMatchDB\")\n",
    "finalWorkingDF[\"words\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 481)\n",
      "['about', 'adam', 'ador', 'advic', 'afraid', 'after', 'again', 'ai', 'air', 'all', 'along', 'alreadi', 'alt', 'am', 'an', 'and', 'angel', 'anoth', 'apart', 'are', 'around', 'arrang', 'as', 'ash', 'ask', 'asleep', 'at', 'aw', 'away', 'back', 'be', 'beach', 'beat', 'been', 'befor', 'believ', 'bend', 'best', 'better', 'bid', 'big', 'bite', 'black', 'bless', 'block', 'bodi', 'bone', 'boss', 'boy', 'breath', 'bridg', 'bright', 'buck', 'burn', 'burnt', 'busi', 'but', 'by', 'ca', 'came', 'can', 'captur', 'car', 'care', 'cast', 'catch', 'caus', 'celebr', 'chanc', 'chang', 'check', 'choos', 'clay', 'clear', 'clock', 'close', 'coast', 'colder', 'color', 'come', 'concern', 'cos', 'could', 'cover', 'cri', 'curl', 'damn', 'dare', 'day', 'de', 'debt', 'deep', 'delight', 'devast', 'die', 'differ', 'din', 'direct', 'dissolv', 'do', 'doe', 'done', 'door', 'doubl', 'down', 'dream', 'drop', 'du', 'each', 'echo', 'edg', 'ei', 'element', 'em', 'en', 'end', 'eras', 'et', 'everi', 'everyth', 'exist', 'eye', 'face', 'fade', 'faith', 'famili', 'famous', 'fashion', 'feel', 'fell', 'felt', 'file', 'find', 'fine', 'fire', 'fix', 'flame', 'fleet', 'flesh', 'fool', 'for', 'forgiv', 'four', 'from', 'fuck', 'game', 'gave', 'get', 'gi', 'girl', 'give', 'go', 'goal', 'god', 'gone', 'good', 'got', 'grace', 'grant', 'grey', 'grind', 'had', 'hand', 'handl', 'happen', 'happi', 'hard', 'has', 'have', 'hear', 'heart', 'heaven', 'hell', 'her', 'here', 'higher', 'highest', 'him', 'his', 'hit', 'hot', 'how', 'ici', 'ideal', 'if', 'ignor', 'in', 'ink', 'inn', 'insid', 'inspir', 'is', 'it', 'jail', 'judg', 'just', 'keep', 'kept', 'kid', 'kill', 'knock', 'know', 'la', 'laid', 'last', 'lay', 'leaf', 'leak', 'leav', 'let', 'leve', 'liar', 'lifeless', 'like', 'limb', 'listen', 'littl', 'live', 'local', 'lock', 'lone', 'long', 'look', 'lot', 'loud', 'love', 'made', 'make', 'man', 'matter', 'may', 'mayb', 'me', 'meant', 'melt', 'memori', 'mess', 'met', 'mic', 'min', 'mine', 'mix', 'mold', 'mom', 'more', 'mot', 'motherfuck', 'mouth', 'much', 'my', 'nake', 'name', 'need', 'never', 'new', 'nigga', 'night', 'no', 'nobodi', 'none', 'noon', 'not', 'now', 'numb', 'number', 'når', 'of', 'off', 'og', 'old', 'older', 'on', 'one', 'onli', 'open', 'or', 'order', 'other', 'our', 'out', 'over', 'paid', 'paint', 'part', 'pass', 'passion', 'past', 'patienc', 'pearl', 'person', 'piec', 'piti', 'place', 'play', 'playa', 'pleas', 'poor', 'pop', 'pray', 'presenc', 'pretti', 'price', 'print', 'process', 'profit', 'push', 'put', 'rap', 'reach', 'readi', 'real', 'realli', 'red', 'relax', 'rememb', 'resist', 'right', 'righteous', 'river', 'rock', 'rose', 'run', 'runaway', 'rust', 'sad', 'sail', 'sale', 'same', 'say', 'seal', 'search', 'season', 'secret', 'see', 'seed', 'seem', 'send', 'set', 'settl', 'shame', 'shark', 'sharp', 'she', 'shine', 'shit', 'shoulder', 'sigh', 'sinc', 'slice', 'small', 'smile', 'so', 'social', 'soil', 'solitud', 'som', 'some', 'somehow', 'someon', 'someth', 'sometim', 'soul', 'space', 'speak', 'spot', 'stain', 'star', 'start', 'stay', 'steadi', 'step', 'stir', 'stone', 'stop', 'street', 'style', 'sun', 'sunset', 'surpris', 'swear', 'så', 'take', 'tell', 'thank', 'that', 'the', 'then', 'there', 'these', 'they', 'thin', 'thing', 'think', 'this', 'thorn', 'those', 'though', 'thought', 'three', 'throne', 'through', 'tide', 'tie', 'tight', 'til', 'till', 'time', 'tip', 'tire', 'to', 'tonight', 'too', 'top', 'torn', 'touch', 'treat', 'tri', 'trip', 'true', 'trust', 'tune', 'turn', 'twice', 'two', 'uh', 'understand', 'up', 'use', 'wait', 'walk', 'wanna', 'want', 'war', 'was', 'wast', 'water', 'wave', 'wax', 'way', 'we', 'wear', 'weed', 'well', 'west', 'wet', 'what', 'when', 'where', 'whi', 'while', 'white', 'who', 'whole', 'wife', 'wilder', 'will', 'win', 'wing', 'wire', 'wise', 'wish', 'wit', 'with', 'wo', 'word', 'work', 'world', 'worn', 'wors', 'worst', 'worth', 'would', 'write', 'yall', 'yeah', 'yellow', 'yes', 'you', 'young', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "\n",
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to the SQLite database\n",
    "        specified by the db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None\n",
    "\n",
    "database = \"mxm_dataset.db\"\n",
    "conn = create_connection(database)\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"SELECT * FROM lyrics\")\n",
    "# count = 0\n",
    "# for row in cur:\n",
    "#     print(row)\n",
    "#     count += 1\n",
    "#     if count==600:\n",
    "#         break\n",
    "\n",
    "# print(\"break\")\n",
    "\n",
    "corpus = []\n",
    "currID = \"bogus\"\n",
    "#vectorizer = CountVectorizer()\n",
    "count = 0\n",
    "newStr = \"\"\n",
    "for row in cur:\n",
    "    if currID != row[0]:\n",
    "        # Add newDict to SKT word thing\n",
    "        if count != 0:\n",
    "            corpus.append(newStr[0:len(newStr)-1])\n",
    "        newStr = row[3]*(row[2]+\" \")\n",
    "        currID = row[0]\n",
    "        count += 1\n",
    "    else:\n",
    "        newStr += row[3]*(row[2]+\" \")\n",
    "        \n",
    "    if count == 10:\n",
    "        break\n",
    "\n",
    "        \n",
    "#X = vectorizer.fit_transform(corpus)\n",
    "#print(vectorizer.get_feature_names())\n",
    "#print(X.toarray())\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "# see final the clean data\n",
    "print (train_data_features.shape)\n",
    "    \n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features, train[\"sentiment\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n",
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect\n",
    "\n",
    "# use TreeankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "vect.set_params(tokenizer=tokenizer.tokenize)\n",
    "\n",
    "# remove English stop words\n",
    "vect.set_params(stop_words='english')\n",
    "\n",
    "# include 1-grams and 2-grams\n",
    "vect.set_params(ngram_range=(1, 2))\n",
    "\n",
    "# ignore terms that appear in more than 50% of the documents\n",
    "vect.set_params(max_df=0.5)\n",
    "\n",
    "# only keep terms that appear in at least 2 documents\n",
    "vect.set_params(min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.merge(genres,on='track_id')\n",
    "usedSongLyrics = pd.DataFrame(columns=['TrackID','MusicMatchID','Word','Count','Training/Testing'])\n",
    "\n",
    "count = 0\n",
    "for row in cur:\n",
    "    if row[0] in trackIDs:\n",
    "        usedSongLyrics.loc[count] = row\n",
    "        count += 1\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
