{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import scipy.io as sio\n",
    "import hdf5_getters\n",
    "from pandas import read_hdf\n",
    "from musixmatch import Musixmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF = pd.read_pickle(\"finalLyricsData\")\n",
    "lyricsTokenDF = pd.read_pickle(\"tokenizedDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'newDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f33fe1babc15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# into feature vectors. The input to fit_transform should be a list of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtrain_data_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lyrics\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Numpy arrays are easy to work with, so convert the result to an\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'newDF' is not defined"
     ]
    }
   ],
   "source": [
    "############## SET UP TOKEN VECTORIZER ##################\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# use TreeankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "vectorizer.set_params(tokenizer=tokenizer.tokenize)\n",
    "\n",
    "# remove English stop words\n",
    "vectorizer.set_params(stop_words='english')\n",
    "\n",
    "# include 1-grams and 2-grams\n",
    "vectorizer.set_params(ngram_range=(1, 2))\n",
    "\n",
    "# ignore terms that appear in more than 50% of the documents\n",
    "vectorizer.set_params(max_df=0.5)\n",
    "\n",
    "# only keep terms that appear in at least 4 documents\n",
    "vectorizer.set_params(min_df=4)\n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(newDF[\"lyrics\"])\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "#train_data_features = train_data_features.toarray()\n",
    "# see final the clean data\n",
    "#print (train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ DIVIDE TRAINING AND TESTING DATA\n",
    "y = lyricsTokenDF['y']\n",
    "x = lyricsTokenDF.drop('y', axis = 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ TRAIN MODELS\n",
    "from sklearn import linear_model\n",
    "regRidge = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0], cv=3)\n",
    "regRidge.fit(x_train, y_train)\n",
    "#RidgeCV(alphas=[0.1, 1.0, 10.0], cv=3, fit_intercept=True, scoring=None, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## TOKENIZE TESTING DATA AND THEN MAKE PREDICTIONS\n",
    "#X_test_vect = vectorizer.transform(x_test)\n",
    "\n",
    "predicted = theModel.predict(x_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
